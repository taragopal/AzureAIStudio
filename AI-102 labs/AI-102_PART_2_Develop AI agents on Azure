Part 1: Get started with AI agent development on Azure
------------------------------------------------------

- Ways developers can create AI agents: 
1. Azure AI Foundry Agent Service is a managed service in Azure that is designed to provide a framework for creating, managing, and using AI agents within Azure
AI Foundry. The service is based on the OpenAI Assistants API
2. OpenAI Assistants API provides a subset of the features in Foundry Agent Service, and can only be used with OpenAI models. 
3. Semantic Kernel is a lightweight, open-source development kit that you can use to build AI agents and orchestrate multi-agent solutions. Semantic kernel SDK
is for GenAI, where Semantic kernel framework is for Agentic solutions. 
4. AutoGen is an open-source framework for developing agents rapidly. It's useful as a research and ideation tool when experimenting with agents.
5. Microsoft 365 Agents SDK: For developers to create self hosted agents
6. Microsoft Copilot Studio provides a low-code development environment for anyone to quickly build and deploy agents that integrate with a Microsoft 365 ecosystem
7. Copilot Studio agent builder in Microsoft 365 Copilot: declarative. Best one for the beginers. 
next one would be MS Power platform
Orgs with more complex extentions can use M365 copilot studio 
For solutions on azure backend use azure AIstudio. Can start with foundry agent and elevate it to the semantic kernels. 


Foundry Agent Service has 3 parts: 
-- Models ( use common OpenAI or Azure AI foundry models)  
-- Knowlege ( enable the agent to ground prompts with contextual data)
-- Tools (Enable code interpretations) 
LAB.2.1: Explore AI Agent development: https://microsoftlearning.github.io/mslearn-ai-agents/Instructions/01-agent-fundamentals.html


Part 2: Develop an AI agent with Azure AI Foundry Agent Service
---------------------------------------------------------------

Knowledge tools
Knowledge tools enhance the context or knowledge of your agent. Available tools include:

Bing Search: Uses Bing search results to ground prompts with real-time live data from the web.
File search: Grounds prompts with data from files in a vector store.
Azure AI Search: Grounds prompts with data from Azure AI Search query results.
Microsoft Fabric: Uses the Fabric Data Agent to ground prompts with data from your Fabric data stores.

Action tools
Action tools perform an action or run a function. Available tools include:

Code Interpreter: A sandbox for model-generated Python code that can access and process uploaded files.
Custom function: Call your custom function code – you must provide function definitions and implementations.
Azure Function: Call code in serverless Azure Functions.
OpenAPI Spec: Call external APIs based on the OpenAPI 3.0 spec.

Lab.2.2: Develop an AI agent: https://microsoftlearning.github.io/mslearn-ai-agents/Instructions/02-build-ai-agent.html


Part 3: Integrate custom tools into your agent
-------------------------------------------------
Azure AI services provide several custom tool options
Customer functions
Azure Functions
OpenAPI specification tools
Azure Logic App 


Example: 
import json

def recent_snowfall(location: str) -> str:
    """
    Fetches recent snowfall totals for a given location.
    :param location: The city name.
    :return: Snowfall details as a JSON string.
    """
    mock_snow_data = {"Seattle": "0 inches", "Denver": "2 inches"}
    snow = mock_snow_data.get(location, "Data not available.")
    return json.dumps({"location": location, "snowfall": snow})

user_functions: Set[Callable[..., Any]] = {
    recent_snowfall,
}


Register the function with your agent using the Azure AI SDK:

# Initialize agent toolset with user functions
functions = FunctionTool(user_functions)
toolset = ToolSet()
toolset.add(functions)
agent_client.enable_auto_function_calls(toolset=toolset)

# Create your agent with the toolset
agent = agent_client.create_agent(
    model="gpt-4o-mini",
    name="snowfall-agent",
    instructions="You are a weather assistant tracking snowfall. Use the provided functions to answer questions.",
    toolset=toolset
)

AZURE FUNCTIONS:::::

#Example: Using Azure Functions with a queue trigger

storage_service_endpoint = "https://<your-storage>.queue.core.windows.net"

azure_function_tool = AzureFunctionTool(
    name="get_snowfall",
    description="Get snowfall information using Azure Function",
    parameters={
            "type": "object",
            "properties": {
                "location": {"type": "string", "description": "The location to check snowfall."},
            },
            "required": ["location"],
        },
    input_queue=AzureFunctionStorageQueue(
        queue_name="input",
        storage_service_endpoint=storage_service_endpoint,
    ),
    output_queue=AzureFunctionStorageQueue(
        queue_name="output",
        storage_service_endpoint=storage_service_endpoint,
    ),
)

agent = agent_client.create_agent(
    model=os.environ["MODEL_DEPLOYMENT_NAME"],
    name="azure-function-agent",
    instructions="You are a snowfall tracking agent. Use the provided Azure Function to fetch snowfall based on location.",
    tools=azure_function_tool.definitions,
)


OpenAPI Specification
#Currently, three authentication types are supported with OpenAPI 3.0 tools: anonymous, API key, and managed identity.
************************************************************************************************
Example: Using an OpenAPI specification
First, create a JSON file ( in this example, called snowfall_openapi.json) describing the API.
************************************************************************************************
{
  "openapi": "3.0.0",
  "info": {
    "title": "Snowfall API",
    "version": "1.0.0"
  },
  "paths": {
    "/snow": {
      "get": {
        "summary": "Get snowfall information",
        "parameters": [
          {
            "name": "location",
            "in": "query",
            "required": true,
            "schema": {
              "type": "string"
            }
          }
        ],
        "responses": {
          "200": {
            "description": "Successful response",
            "content": {
              "application/json": {
                "schema": {
                  "type": "object",
                  "properties": {
                    "location": {"type": "string"},
                    "snow": {"type": "string"}
                  }
                }
              }
            }
          }
        }
      }
    }
  }
}

Simple format of an API: 1.  declaration of api and metadata.
     2. paths: method = get(), parameters(name, in, required, schema type)
     3. response : 200(), content -> schema -> properties

#Then, register the OpenAPI tool in the agent defintion:
--------------------------------------------------------------------

from azure.ai.agents.models import OpenApiTool, OpenApiAnonymousAuthDetails

with open("snowfall_openapi.json", "r") as f:
    openapi_spec = json.load(f)

auth = OpenApiAnonymousAuthDetails()
openapi_tool = OpenApiTool(name="snowfall_api", spec=openapi_spec, auth=auth)

agent = agent_client.create_agent(
    model="gpt-4o-mini",
    name="openapi-agent",
    instructions="You are a snowfall tracking assistant. Use the API to fetch snowfall data.",
    tools=[openapi_tool]
)

LAB.2.3: Build an agent with custom tools 

*************************************************************************************************************************************************
Part 4: Develop a multi-agent solution with Azure AI Foundry Agent Service
---------------------------------------------------------------------------

Describe how connected agents enable modular, collaborative workflows.
Design a multi-agent solution by defining main agent tools and connected agent roles.
Build and run a connected agent solution

What? Azure AI Foundry Agent Service can help you develop sophisticated, multi-agent systems that can break down complex tasks into smaller, specialized roles.
Using connected agents, you can design intelligent solutions where a primary agent delegates work to sub-agents without the need for custom orchestration logic
or hardcoded routing. This modular approach boosts efficiency and maintainability across a wide range of scenarios.


Why ? Can help modularize the big task, without using an orchestrator. Its very flexible, because of its modularity. Using agents becomes more easy for traceability. 
Everyworkflow is more reliable and easier to troubleshoot incase of a problem. 


Steps to set up a multi-agent solution with connected agents: 

1. Initialize the agents client: First, you create a client that connects to your Azure AI Foundry project.
2. Create an agent to connect to the main agent: Define an agent you want to connect to the main agent. You can do this using the create_agent method on the 
AgentsClient object. 
For example, your connected agent might retrieve stock prices, summarize documents, or validate compliance. Give the agent clear instructions that define its 
purpose.
3. Initialize the connected agent tool: Use your agent definition to create a ConnectedAgentTool. Assign it a name and description so the main agent knows when 
and how to use it.
4. Create the main agent: Create the main agent using the create_agent method. Add your connected agents using the tools property and assign the ConnectedAgentTool 
definitions to the main agent.
5. Create a thread and send a message: Create the agent thread that is used to manage the conversation context. Then create a message on the thread that contains 
the request you want the agent to fulfill.
6. Run the agent workflow: Once the message is added, create a run to process the request. The main agent uses its tools to delegate tasks as needed and compile a 
final response for the user.
7. Handle the results: When the run completes, you can review the main agent's response. The final output may incorporate insights from one or more connected agents. Only the main agent's response is visible to the end user.

LAB.2.4: Develop a multi-agent solution: https://microsoftlearning.github.io/mslearn-ai-agents/Instructions/03b-build-multi-agent-solution.html

********************************************************************************************************************************************************************
Part 5: Integrate MCP Tools with Azure AI Agents

Explain the roles of the MCP server and client in tool discovery and invocation.
Wrap MCP tools as asynchronous functions and register them with Azure AI agents.
Build an Azure AI agent that dynamically accesses and calls MCP tools during runtime.


What is MCP Tools?  
-------------------
Connecting an Azure AI Agent to a Model Context Protocol (MCP) server can provide your agent with a catalog of tools accessible on demand.
This approach makes your AI solution more robust, scalable, and easier to maintain.

What is dynamic tool discovery?
--------------------------------
Dynamic tool discovery is a mechanism that allows an AI agent to discover available external tools without needing hardcoded knowledge of each one. Instead of 
manually adding or updating every tool your agent can use, the agent queries a centralized Model Context Protocol (MCP) server. This server acts as a live catalog,
exposing tools that the agent can understand and call.

How does MCP enable dynamic tool discovery?
--------------------------------------------
An MCP server hosts a set of functions that are exposed as tools using the @mcp.tool decorator. Tools are a primitive type in the MCP that enables servers to
expose executable functionality to clients. A client can connect to the server and fetch these tools dynamically. The client then generates function wrappers 
that are added to the Azure AI Agent's tool definitions. This setup creates a flexible pipeline:

I. The MCP server hosts available tools.
II. The MCP client dynamically discovers the tools.
III. The Azure AI Agent uses the available tools to respond to user requests.

What is the MCP Server?
------------------------
The MCP server acts as a registry for tools your agent can use. You can initialize your MCP server using FastMCP("server-name"). The FastMCP class uses Python
type hints and document strings to automatically generate tool definitions, making it easy to create and maintain MCP tools. These definitions are then served 
over HTTP when requested by the client. Because tool definitions live on the server, you can update or add new tools at any time, without having to modify or
redeploy your agent.

What is the MCP Client?
--------------------------
A standard MCP client acts as a bridge between your MCP server and the Azure AI Agent Service. The client initializes an MCP client session and connects to the
server. Afterwards, it performs three key tasks:

Discovers available tools from the MCP server using session.list_tools().
Generates Python function stubs that wrap the tools.
Registers those functions with your agent.
This allows the agent to call any tool listed in the MCP catalog as if it were a native function, all without hardcoded logic.

Hpw and when to register AI tools with an Azure AI Agent? 
----------------------------------------------------------
When an MCP client session is initialized, the client can dynamically pull in tools from the MCP server. An MCP tool can be invoked using 
session.call_tool(tool_name, tool_args). 
The tools should each be wrapped in an async function so that the agent is able to invoke them. Finally, those functions are bundled together and become part of 
the agent's toolset and are available during runtime for any user request.

Can you give an overview of MCP agent tool integration ? 
---------------------------------------------------------
I. The MCP server hosts tool definitions decorated with @mcp.tool.
II. The MCP client initializes an MCP client connection to the server.
III. The MCP client fetches the available tool definitions with session.list_tools().
IV. Each tool is wrapped in an "(?)async function" that invokes session.call_tool
V. The tool functions are bundled into FunctionTool that makes them usable by the agent.
VI. The FunctionTool is registered to the agent's toolset.

Now your agent is able to access and invoke your tools through natural language interaction. By setting up the MCP server and client, you create a clean
separation between tool management and agent logic—enabling your system to adapt quickly as new tools become available.

What do we need to connect to an MCP server ? 
----------------------------------------------
A remote MCP server endpoint (for example, https://api.githubcopilot.com/mcp/).
An Azure AI Foundry agent configured to use the MCP tool.
You can connect to multiple MCP servers by adding them as separate tools, each with:

: server_label: A unique identifier for the MCP server (e.g., GitHub).
: server_url: The MCP server’s URL.
: allowed_tools (optional): A list of specific tools the agent is allowed to access.

The MCP tool also supports custom headers, which let you pass:
- Authentication keys (API keys, OAuth tokens).
- Other required headers for the MCP server.
These headers are included in tool_resources during each run and are not stored between runs.

How the MCP tools are involked?  
-----------------------------------
When using the Azure MCP Tool object, you don't need to wrap function tools or invoke session.call_tool. Instead, the tools are automatically invoked when 
necessary during an agent run. To automatically invoke MCP tools:

- Create the McpTool object with the server label and url.
- Use update_headers to apply any headers required by the server.
- Use the set_approval_mode to determine whether approval is required. Supported values are:
    always: A developer needs to provide approval for every call. If you don't provide a value, this one is the default.
    never: No approval is required.
    Create a ToolSet object and add the McpTool object
- Create an agent run and specify the toolset property
- When the run completes, you should see the results of any invoked tools in the response.
If the model tries to invoke a tool in your MCP server with approval required, you get a run status of requires_action. - In the requires_action field, you 
can get more details on which tool in the MCP server is called and any arguments to be passed. - Review the tool and arguments so that you can make an informed 
decision for approval. - Submit your approval to the agent with call_id by setting approve to true.

MCP integration is a key step toward creating richer, more context-aware AI agents. As the MCP ecosystem grows, you’ll have even more opportunities to bring 
specialized tools into your workflows and deliver smarter, more dynamic solutions.

LAB.2.5: Connect AI agents to tools using Model Context Protocol (MCP): https://microsoftlearning.github.io/mslearn-ai-agents/Instructions/03c-use-agent-tools-with-mcp.html

**Futher Reading: **
Lesson 11: Model Context Protocol (MCP) Integration: https://github.com/microsoft/ai-agents-for-beginners/blob/main/11-mcp/README.md
https://learn.microsoft.com/en-us/azure/ai-foundry/agents/how-to/tools/model-context-protocol
Intro to MCP : https://modelcontextprotocol.io/docs/getting-started/intro


********************************************************************************************************************************************************************
Part 6: Develop an AI agent with Semantic Kernel
Check no.16 for the details on SKAgent


Part 7: Orchestrate a multi-agent solution using Semantic Kernel
- Use the applied skill list for the detailed understanding 
https://learn.microsoft.com/en-us/credentials/applied-skills/develop-ai-agents-using-microsoft-azure-openai-and-semantic-kernel/ 






